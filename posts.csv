4,1,A Rite of Passage: Compiling Markdown,"Odds are, if you're a web developer like me, you've tried to build a developer blog. And if you've done that before, you've probably experienced the *delight* that is Markdown parsing and compilation. But it's never just Markdown parsing, it's about everything else surrounding it. ","<!-- ---
title: ""A Rite of Passage: Compiling Markdown""
tags: Remix, Rust, WASM, JS, Markdown, femark
---
-->
## Intro

Odds are, if you're a web developer like me, you've tried to build a developer blog. And if you've done that before, you've probably experienced the *delight* that is Markdown parsing and compilation. But it's never just Markdown parsing, it's about everything else surrounding it. Extracting front matter, highlighting code blocks, dealing with tables, generating a table of contents, dealing with embeds, and more. An entire ecosystem of packages have sprung up to deal with it, each with their own requirements and limitations. Parsing all that is not easy, even for experienced devs.

<div class=""flex w-full justify-center""> 
<a href=""https://twitter.com/tannerlinsley/status/1527752952768696320?s=20&t=mPPeKvolAeJJyydQxoRdyQ"" >
<img src=""https://benwis.imgix.net/tanner-linsley-md-tweet.png?fm=png&auto=format"" alt=""Tanner Linsley decrying the state of Markdown packages""/>
</a>
</div>

> Shouldn't you be using [Portable Text](https://www.smashingmagazine.com/2022/02/thoughts-on-markdown/)?

Shh. That's even harder to setup outside of the Sanity ecosystem.

## Performance Considerations

As you can imagine, all that parsing is not particularily fast. And because it's not that fast, we've got to consider where and when you'd want to do it. I've seen a variety of solutions. Discord Remix User Kilman processes markdown once and stores it to a file. Amos processes it and stores it in an sqlite database. Kent C. Dodds processes it and then has the browser cache the result by setting cache control headers. All of these approaches suggest we're concerned about the time this takes, so let's put some numbers on that concern.

## The Contenders

I spent some time scouring the JS ecosystem for the most common packages used for the task. I believe I've found the most common ones, but if I haven't please ping me on Twitter.

- unified, remark, and rehype-highlight ecosystem
  - Arguably one of the most popular and common ecosystems for parsing markdown into in HTML in JS, it features a huge number of packages and plugins designed to handle every possible option. It's an ecosystem more than a single package, to get where we need to go we'll need a lot of different packages.
- marked and highlight.js
  - marked claims it is ""Built for Speed"", and while this doesn't have as many features as the above, it can do everything we need with a lot fewer packages. It's also pure JS, so it is very portable.

## The Test

For this test, I've chosen to compile a blog post from the prolific blog [fasterthanlime](https://fasterthanli.me/)! If you haven't heard of amos, he writes very, very long form content about Rust, Go, static typing, dev ops, and so much more. Definitely worth a read if you have a spare hour or two. He's graciously provided one of his posts, [The Curse of Strong Typing](https://fasterthanli.me/articles/the-curse-of-strong-typing), in markdown format, and agreed to make the [file available](https://gist.github.com/fasterthanlime/c581578d2cd6dd71bb7250c5a1d2ba75) to everyone. Coming in at a whopping 20,395 words and 7,645 lines, with 378 code blocks, you'd be hard pressed to find a better challenge for a markdown compiler outside of a book. 

### Test Environment

Because I've been using Remix a lot lately, and because I just rewrote my blog with it, I'm going to test this using Remix. I'll time the time it takes before it returns html with syntax highlighted code blocks. I'll try to keep the output equivalent for each option, so no html sanitization, table of contents generation, frontmatter parsing, or anything else. This is running locally on my M1 13"" Macbook Pro, which should give the best case performance. But we're interested in the differences anyway. I'll run each test independently ten times, and then average the results. 

For those unfamiliar with Remix and the loader paradigm, loaders are rendered on the server when the page loads. They then return json or other data in an HTML [response](https://developer.mozilla.org/en-US/docs/Web/API/Response) that is then displayed by the client.

Below is the loader from the page that renders Markdown content on my post page.

```tsx
import { marked } from ""marked"";
import hljs from 'highlight.js';
import rust from 'highlight.js/lib/languages/rust';
import codeStyles from 'highlight.js/styles/github.css';
import { markdownToHtml } from ""~/models/markdown.server"";
...

export const loader: LoaderFunction = async ({
    params, request
  }) => {
    invariant(params.slug, `params.slug is required`);
    const post = await getPost(params.slug);
    invariant(post, `Post not found: ${params.slug}`);

    // Set options
    hljs.registerLanguage('rust', rust);

    marked.setOptions({
        renderer: new marked.Renderer(),
        highlight: function(code, lang) {
          const language = hljs.getLanguage(lang) ? lang : 'plaintext';
          return hljs.highlight(code, { language }).value;
        },
        langPrefix: 'hljs language-', // highlight.js css expects a top-level 'hljs' class.
        pedantic: false,
        gfm: true,
      });

    const markedStart = performance.now();
    marked.parse(post.markdown);
    const markedEnd = performance.now();
    console.log(`Marked Time: ${markedEnd - markedStart}ms`);

    const remarkStart = performance.now();
    await markdownToHtml(post.markdown);
    const remarkEnd = performance.now();
    console.log(`Remark Time: ${remarkEnd - remarkStart}ms`);

    return json<LoaderData>({ admin, post, html: html });
};
```

> Wait a minute, where's the Remark code?

Well, as it turns out Remark's ecosystem is a lot more involved, so I extracted it to its own file. Here it is, shamelessly cribbed from the venerable Kent C. Dodds.

```tsx
async function markdownToHtml(markdownString: string) {
    const { unified } = await import('unified')
    const { default: markdown } = await import('remark-parse')
    const { default: remark2rehype } = await import('remark-rehype')
    const { default: rehypeStringify } = await import('rehype-stringify')
    const { default: rehypeHighlight } = await import('rehype-highlight')

    const result = await unified()
        .use(markdown)
        .use(remark2rehype)
        .use(rehypeStringify)
        .use(rehypeHighlight, { ignoreMissing: true, aliases: { 'none': 'text' } })
        .process(markdownString)

    return result.value.toString()
}

export {
    markdownToHtml,
    markdownToHtmlUnwrapped,
}
```

> If everybody is caching the output or storing it in a DB, why do we care about any of this?

That's not very helpful, but maybe you'd like to render a split page post editor with a live preview. Or you care about reducing the environmental impact of your blog posts. Or maybe you just want bragging rights. Anyway, on to the results!

### Results

![Performance graph showing marked and remark, with marked handily winning](https://benwis.imgix.net/js-md-compile-times.png?fm=png&auto=format)

Clearly, the winner here is marked, handily beating the remark ecosystem in every test. I guess it's truly built for speed after all.

> Aren't you missing something? Don't you know a way this could be faster?

No, not really. These are the current best options in JS. What could you mean?

> Maybe you're feeling a little bit... crabby? ü¶Ä

Alright fine, let's see if I can do this faster in Rust!

### Rust Contenders

As it turns out, Rust has several good crates for markdown to HTML compilation, the most popular being [pulldown-cmark](https://crates.io/crates/pulldown-cmark) and [comrak](https://crates.io/crates/comrak). It also has a popular syntax highlighter called [syntect](https://crates.io/crates/syntect) that was developed for Sublime Text. But I did not find any npm packages that use these, so there's not really a way to compare them...

> Giving up so easily?

Ok fine. I'll just make my own. Let's grab pulldown-cmark and syntect, compile it using napi-rs and wasm-bindgen into an npm package, and see how they shake out. I shall call the package femark.

![Performance graph of femark-napi, femark, remark, and marked options](https://benwis.imgix.net/md-compile-times-fp.png)

In this graph, the femark package is compiled to WASM, and femark-napi is compiled as a native Rust module. And would you look at that, both options handily beat remark. That's progress! But I'm disappointed that the native Rust version is roughly on par with marked, and femark loses to marked every time. Isn't Rust supposed to be faster than JS?

> You've fallen victim to one of the classic blunders!

A land war in Asia?

> No, silly. What do you think Node.js uses to do Regular Expressions?

It's written in C isn't it.

> [Yep](https://github.com/v8/v8/tree/main/src/regexp)

highlight.js uses regexes, just like syntect, but those regex calls are just a thin wrapper around Node's v8 engine. Since Rust's native performance is roughly equivalent to C, the two options are roughly equivalent in speed. And WASM experiences various performance penalties and overhead copying data in and out, so it won't be faster either. Well played, Node/Browser devs.

### The End?

At this point, feeling a bit miffed,I reached out to amos and asked him how he parses and highlights his markdown. And he mentioned that he much preferred using [tree-sitter](https://crates.io/crates/tree-sitter) over syntect, because it ""uses actual parsers, not regex soup"". tree-sitter has worked wonders in my neovim environment, but I hadn't heard about anyone using it on the web before. I also couldn't find any performance comparisons between a regex and parser implementation. I suspect that it might be faster, so let's check it out. 

![Run time comparison between the packages. femark-ts handily beats all the options](https://benwis.imgix.net/md-highlight-times-sp.png?fm=png&auto=format)

There we go, femark-ts is the treesitter version compiled with napi-rs, and it handily trounces the syntect version and marked by about 3x and the remark one by 20x! 

## Conclusion

This whole experience is a perfect lesson that just because you rewrite an npm package in Rust, it does not automatically make it faster. One needs to analyze what the JS version does, and whether it just calls out to C.  A good design, written in a fast language, will be faster than a good design in JS, unless that JS has help.

If you're interested in quickly compiling your markdown to HTML and syntax highlighting it, I've published the fastest verion of the package, previously referred to as femark-ts, to npm as [femark](https://www.npmjs.com/package/@benwis/femark). Not only is it blazingly fast, it also uses classes instead of style tags for brevity and customization. Check it out, [PRs and comments welcome!](https://github.com/benwis/femark)

### Thanks

A big thanks to Amos for providing the markdown of his post, and guidance. This post wouldn't be possible without the hard work of developers in the Rust lang, tree-sitter, pulldown-cmark, syntect, napi-rs, and wasm-bindgen projects. And others, many many others.

### Average Run Times

| marked           | femark-napi      | femark-ts        | remark           | femark           |
| ---------------- | ---------------- | ---------------- | ---------------- | ---------------- |
| 55.3464833855629 | 48.9104747891426 | 14.9385375857353 | 329.070762491226 | 144.905074894428 |

### Raw Data

| marked             | femark-napi        | femark-ts          | remark             | femark             |
| ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| 52.5864169597626   | 47.04791700840000  | 13.743250012397800 | 192.7639158964160  | 202.89374995231600 |
| 66.54324996471410  | 52.11045789718630  | 22.594791889190700 | 408.9021250009540  | 155.1495840549470  |
| 63.25125002861020  | 47.306707978248600 | 14.036875009536700 | 286.07187509536700 | 132.7103749513630  |
| 51.76112496852880  | 47.231040954589800 | 13.419041991233800 | 346.07716703414900 | 133.82504105567900 |
| 51.40050005912780  | 48.94004201889040  | 14.00616705417630  | 363.82941591739700 | 139.92316699028000 |
| 40.90349996089940  | 46.65362501144410  | 13.58062493801120  | 301.24004209041600 | 135.77149999141700 |
| 46.893749952316300 | 49.40858292579650  | 14.68291699886320  | 319.53983294963800 | 134.85695803165400 |
| 61.449041962623600 | 53.4950829744339   | 14.871457934379600 | 364.0340839624410  | 137.96595799922900 |
| 50.269083976745600 | 48.28879106044770  | 14.132166981697100 | 341.6937919855120  | 138.78633296489700 |
| 68.40691602230070  | 48.62250006198880  | 14.318083047866800 | 366.55537497997300 | 137.1680829524990  |





",,compiling-markdown,1,1,"",,,1655769600,1680129798
5,1,Serving Static Files With Axum,"Perhaps you'd like to run a fileserver with Axum, or maybe just serve static content in a folder. It's a bit trickier in Axum than you might expect","<!-- ---
title: ""Serving Static Files With Axum""
tags: Axum, files, static, hot_tip

---
-->

## Intro

Perhaps you'd like to run a fileserver with Axum, or maybe just serve static content in a folder. It's a bit trickier in Axum than you might expect.

## Background

I was recently converting an example for [webauthn-rs](https://github.com/kanidm/webauthn-rs) from tide to axum, because I really liked that it uses the types of the http crate and interoperates easily with tower layers. But I hit a snag.

The server runs a Webassembly client library in a server rendered page, which means it needs to provide a `.wasm` binary file and the JS glue code to run it. 

For those curious, this is the function that does that. It returns an HTML document with a `<script>` tag that asynchronously loads the web assembly binary file.

```rust
async fn index_view(_request: tide::Request<AppState>) -> tide::Result {
    let mut res = tide::Response::new(200);
    res.set_content_type(""text/html;charset=utf-8"");
    res.set_body(
        r#""
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1"">
    <title>WebAuthn-rs Tutorial</title>
    <script type=""module"">
        import init, { run_app } from './pkg/wasm.js';
        async function main() {
           await init('./pkg/wasm_bg.wasm');
           run_app();
        }
        main()
    </script>
  </head>
  <body>
  </body>
</html>
    ""#,
    );
    Ok(res)
}
```

For more details on how to load Webassembly files in Rust, checkout this detailed [blog post](https://tung.github.io/posts/rust-and-webassembly-without-a-bundler/) from Tung's Word Box.

In tide, the route responsible for serving this file looks like this:

```rust
    let mut app = tide::with_state(app_state);
		...
    // Serve our wasm content
    app.at(""/pkg"").serve_dir(""../../wasm/pkg"")?;
		...
```

You might think, as I have, that there would be an equivalent of `.serve_dir()` in Axum, but you would be mistaken.

## Serve Files with Axum

Since there's no easy handler, we've got a bit of work to do. First, we need to define a function that will get the file from the filesystem , and returns a response with the file contents in byte format. Thankfully, the tower ecosystem has a handy helper in `tower_http` called [ServeDir](https://docs.rs/tower-http/latest/tower_http/services/struct.ServeDir.html). Just add it to your `cargo.toml`, and don't forget the `fs` feature flag! Thus, we can build the following function to create that response:

```rust
async fn get_static_file(uri: Uri) -> Result<Response<BoxBody>, (StatusCode, String)> {
    let req = Request::builder().uri(uri).body(Body::empty()).unwrap();

    // `ServeDir` implements `tower::Service` so we can call it with `tower::ServiceExt::oneshot`
    // When run normally, the root is the workspace root
    match ServeDir::new(""../../wasm/pkg"").oneshot(req).await {
        Ok(res) => Ok(res.map(boxed)),
        Err(err) => Err((
            StatusCode::INTERNAL_SERVER_ERROR,
            format!(""Something went wrong: {}"", err),
        )),
    }
}
```

You can see that we are distributing files from the filepath indicated in the `new()` function, and that it is relative to the root where you run your app.

Now that we have that, we can write a handler to take in a URI:

```rust
pub async fn file_handler(uri: Uri) -> Result<Response<BoxBody>, (StatusCode, String)> {
    let res = get_static_file(uri.clone()).await?;

    if res.status() == StatusCode::NOT_FOUND {
        // try with `.html`
        // TODO: handle if the Uri has query parameters
        match format!(""{}.html"", uri).parse() {
            Ok(uri_html) => get_static_file(uri_html).await,
            Err(_) => Err((StatusCode::INTERNAL_SERVER_ERROR, ""Invalid URI"".to_string())),
        }
    } else {
        Ok(res)
    }
}
```

Now we just need to add a route in our Axum router like so:

```rust
 let app = Router::new()
     .nest(""/pkg"", get(file_handler));
```

`.nest()` collects all the file paths off of the parent, making it perfect for distributing a particular file.And that's it, 24 lines of code plus an external tower service to match Tide's `.serve_dir()` helper. But hey, it works just fine! And ServeDir is neat because you can call it inside of any other handler.

### Thanks

Big thank you to @davidpdrsn on github for posting his version in this github [discussion](https://github.com/tokio-rs/axum/discussions/446), which only needed some upgrades for various changes in types.

#### Full Code:

```rust
use axum::{
    body::{boxed, Body, BoxBody},
    http::{Request, Response, StatusCode, Uri},
};
use tower::ServiceExt;
use tower_http::services::ServeDir;

pub async fn file_handler(uri: Uri) -> Result<Response<BoxBody>, (StatusCode, String)> {
    let res = get_static_file(uri.clone()).await?;
    println!(""{:?}"", res);

    if res.status() == StatusCode::NOT_FOUND {
        // try with `.html`
        // TODO: handle if the Uri has query parameters
        match format!(""{}.html"", uri).parse() {
            Ok(uri_html) => get_static_file(uri_html).await,
            Err(_) => Err((StatusCode::INTERNAL_SERVER_ERROR, ""Invalid URI"".to_string())),
        }
    } else {
        Ok(res)
    }
}

async fn get_static_file(uri: Uri) -> Result<Response<BoxBody>, (StatusCode, String)> {
    let req = Request::builder().uri(uri).body(Body::empty()).unwrap();

    // `ServeDir` implements `tower::Service` so we can call it with `tower::ServiceExt::oneshot`
    // When run normally, the root is the workspace root
    match ServeDir::new(""./webauthn_client/pkg"").oneshot(req).await {
        Ok(res) => Ok(res.map(boxed)),
        Err(err) => Err((
            StatusCode::INTERNAL_SERVER_ERROR,
            format!(""Something went wrong: {}"", err),
        )),
    }
}

```





",,serving-static-files-with-axum,1,1,"",,,1660780800,1680129760
6,1,The Gist of gRPC,"Get the gist of gRPC, the high throughput, lightweight, and multilanguage API framework! Great for projects that do video, streaming, and more!","<!-- ---
title: ""The Gist of gRPC""
tags:  grpc, tutorial
---
-->
## Why?

Recently the Twitterverse has been abuzz with discussion about the drawbacks of GraphQL, the benefits of regular old REST, and how amazing tRPC is. By default, because my backends are often written in Rust, I've been using GraphQL. And for the most part it's OK, but recently I've been wondering if we can't do better. And that has led me to gRPC or **g**RPC **R**emote **P**rocedure **C**alls*(It's a recursive acronym!)*, the seemingly little known API framework used by such big names as Uber, Square, Docker, Cisco, CockroachDB, Spotify, and Google.

<img style=""height: 20rem; margin: auto;"" src=""https://benwis.imgix.net/grpc-icon-color-blackbg.png"" alt=""gRPC logo on black background"">

gRPC offers high throughput, low latency communication over HTTP/2, is very lightweight, offers bi-directional streaming, and [7-10x performance improvements over a REST+JSON implementation](https://medium.com/@EmperorRXF/evaluating-performance-of-rest-vs-grpc-1b8bdf0b22da#:~:text=gRPC%20is%20roughly%207%20times,of%20HTTP%2F2%20by%20gRPC.). It also generates language bindings from a `.proto` file with it's own IDL, and is completely open source. All in all, it sounds a LOT like Prisma, but with some nice perf benefits. Let's see how that works out.

## Core Concepts

### Remote Procedure Call (RPC)

<img style=""height: 20rem; margin: auto;"" src=""https://benwis.imgix.net/rpc_diagram.jpeg"" alt=""rpC call from client which calls a skeleton(stub), which calls a RPC protocol, which is received on the server. The server then processes the request, runs the function, and returns the response through the inverse method."">

A Remote Procedure Call is a method for building distributed systems. It allows one to define a function(also called a service in gRPC or a stub/skeleton) on one machine, and then for remote clients to call it as if it was a native function on the caller's device.  Crucially, this does not mean that a local call is the same as a remote one, with additional processing to handle its operation over the wire. There are many different flavors of RPC, starting with Sun RPC in the 1980s(used to build NFS), and more modern examples like tRPC or gRPC. For gRPC, the functions, their inputs, and the possible outputs are all defined using Google's Protocol Buffers spec.

### Protocol Buffers

<img style=""height: 20rem; margin: auto;"" src=""https://benwis.imgix.net/protocol_buffers_large.png"" alt=""The Protocol Buffer logo, with the text Binary Serialization with Protocol Buffers"">

For those unfamiliar, [Protocol Buffers](https://developers.google.com/protocol-buffers) are a language neutral, platform agnostic and extensible mechanism for serializing structured data. One can define their API's functions, requests, and responses in a simple structured language. It has a wide variety of possible scalar values, to encode anything from a `uint32` to `bytes` or a `String`. This gives some nice flexibility over GraphQL.

When sent over the network, the data is serialized into a binary form, and deserialized at the server. The message format is not self-descriptive, both servers and clients must have the same protocol buffer definition. Special care must be taken not to replace existing messages or field order if there's a chance of schema mismatch.

Bonus points if you can tell who wrote the protocol buffer spec from the logo ;).

<blockquote class=""twitter-tweet""><p lang=""en"" dir=""ltr"">The &quot;you get i32 or f64 for numbers&quot; is a graphql decision unfortunately</p>&mdash; Sage Griffin üè≥Ô∏è‚Äç‚ößÔ∏èüè≥Ô∏è‚Äçüåà (@sgrif) <a href=""https://twitter.com/sgrif/status/1569451990916435971?ref_src=twsrc%5Etfw"">September 12, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script>

Here's an example `.proto` file where we can define a protocol.

```protobuf
syntax = ""proto3"";
package helloworld;

service Greeter {
    rpc SayHello (HelloRequest) returns (HelloReply); //unary
    rpc LotsOfReplies(HelloRequest) returns (stream HelloResponse); //Server side streaming
    rpc LotsOfGreetings(stream HelloRequest) returns (HelloResponse); // Client Side streaming
	rpc BiDiGreetings(stream HelloRequest) returns (stream HelloResponse); // bidirectional

}

message HelloRequest {
   string name = 1;
   float decimal = 2;
   bytes buffer = 3;
}

message HelloReply {
    string message = 1;
}
```

*Here is an example ""protobuf"" file from [tonic](https://github.com/hyperium/tonic)'s simple [hello world example](https://github.com/hyperium/tonic/blob/master/examples/helloworld-tutorial.md).*

#### Preamble

The first two lines are used by Tonic to determine details about the Protocol Buffers. 
`syntax=""proto3""` tells us that it is using v3 of the Protocol Buffer spec, and `package helloworld` tells Tonic the name of the package that we can import later.

#### Service Methods

gRPC has four ""service methods"", or types of functions in our api.

1. Unary RPCs - Denoted by the `rpc` keyword, these are typical functions where a single request is sent to the server and a single response is returned. SayHello in the above example is a unary RPC.
2. Server Streaming RPC - Has the `rpc` keyword, and returns a `stream`. This allows the server to send a stream or multiple messages in response to a request. The client reads from the stream until there are no more responses, and the message order is preserved. LotsofReplies is a Server Streaming RPC.
3. Client Streaming RPC - Has the rpc keyword, and takes a stream as an input. The client sends a stream of messages and the server returns a single response. LotsOfGreetins is a Client Streaming RPC
4. Bidirectional Streaming RPC - Has the `rpc` keyword, and takes a `stream` as an input and an output. Both sides send a sequence of messages using a read/write `stream`, in whatever order the client/server would like. BiDiGreetings is a Bidirectional Streaming RPC.

gRPC is unique in that a deadlines can be set on the client and a timeout on the server, a request can be cancelled at any time, and 

More detail about these methods can be found in the [gRPC docs](https://grpc.io/docs/what-is-grpc/core-concepts/#metadata).

```protobuf
message HelloRequest {
   string name = 1;
   float decimal = 2;
   bytes buffer = 3;
}

message HelloReply {
    string message = 1;
}
```

#### Messages

These are request and response objects passed into and returned from service methods. They are numbered, and serialized before being passed to the server. `HelloRequest` and `HelloResponse` are messages with a string type, check out [this list](https://developers.google.com/protocol-buffers/docs/proto3#scalar) for all the types! While the Rust types aren't listed in this table, they are a fairly straightforward mapping from the included C ones.

#### Metadata

Metadata are key value pairs that can pass additional data to the gRPC server or client, and are opaque to gRPC itself. For example, authentication tokens can be passed as metadata from client to server. These are not usually defined in the `.proto` file and are handled in your resolvers.

#### Channels

A channel provides a connection to a specific gRPC server and port. Channels have different configuration options when they're opened, and accept metadata as well. See your gRPC server docs for available options. 

#### Compilation

Protobuf files are compiled into language bindings with a language dependent compiler. For Rust that compiler is called `prost`, a handy Rust wrapper around the [Protocol Buffers library](https://github.com/protocolbuffers/protobuf). Other languages might use `protoc` or their own specific implementation.

#### Interoperability 

One of the neat things about Protocol Buffers is that the file can be reused for all the other clients and servers in your model. If you have an IOS app, you can pass it your server's `.proto` file and write your own client functions. Or a web site, or another server, or a microcontroller. Possibilities abound! For Rust, Tonic, can handle generating both server and client bindings in the same crate. 

## Time for an Example!

Now that we know the basics, let's use it to build a gRPC server in Rust with [Tonic](). 

gRPC API development follows 2 basic steps

1. Define your messages(Service method inputs and responses) and service methods(functions) in your  `.proto` file. These will get compiled into Rust language bindings using [prost](https://github.com/tokio-rs/prost). Other languages will use [protoc]() or their own implementation 
2. Write your resolvers in your language of choice.

If you're familiar with the GraphQL world, this would be a schema-first approach. To my knowledge, there are no code-first gRPC packages, but it's broad language support suggests there could be something out there.

### Hello World in gRPC

I thought about writing my own version of a hello world in gRPC, but ultimately I discovered there wasn't much to add over the existing resources. Check out the links below for gRPC Hello World examples in your language of choice

* [Rust](https://github.com/hyperium/tonic/blob/master/examples/helloworld-tutorial.md)
* [Typescript](https://dev.to/devaddict/use-grpc-with-node-js-and-typescript-3c58) 
* [C#, C++, Dart, Go, Java, Kotlin, Node, Objective-C, PHP, Python, Ruby](https://grpc.io/docs/languages/)


Once you have the server up, you should be able to query it using [Postman](https://www.postman.com/), who has announced native support, or an open source alternative called [BloomRPC](https://github.com/bloomrpc/bloomrpc). If you're a curl lover, there's also [grpcurl](https://github.com/fullstorydev/grpcurl).

I recommend checking out the [official gRPC website](https://grpc.io/) for other resources and additional info!

## gRPC-web

Up to this point, I hope I've enticed you into trying gRPC, but unfortunately it's not all sunshine and roses. gRPC works great if you are communicating between two gRPC servers/clients(like a mobile app and an API server, or a file server and an API server), but it has some limitations in the browser itself. Primarily this is because browsers do not currently have support for streaming from the client, and somewhat limited HTTP trailer support. Check out [this issue](https://github.com/whatwg/fetch/issues/1438) for progress on that front. To work on that, Google is developing gRPC-web, which has a bit more limited feature set.

It only supports unary and server streaming RPCs, and there needs to be a proxy of some sort running between your gRPC server and your browser client. The typical choice for that is [Envoy](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/other_protocols/grpc). In Rust, there's a handy `tonic-web` crate that eliminates the need for the proxy. Other languages may have similar solutions, or a grpc-web native server solution.

## Conclusion

gRPC seems to be little known among the web development community, but has widespread adoption among large companies for server/app/mobile communications. With the advent of gRPC-web, I believe it's a viable option to replace REST or GraphQL in applications that desire high throughput, low latency, and language flexibility.  Its ability to handle streaming in different configurations suggests it might be a viable replacement for Web Sockets as well. 

I'm excited to try it on some of our sites, and maybe on some embedded devices(ESP32 anyone?). If I've made any mistakes, or you want to show off something you've built using gRPC, or anything else, feel free to reach out on Twitter! I love hearing from y'all

",,the-gist-of-grpc,1,1,"",,,1660867200,1680129700
7,1,ReNixing My OS,"Have you heard of Nix and NixOS? Do you dream of config files? Learn how to use NixOS to make a better dev workstation, and you the greatest, most interesting, most important developer in the realm","<!-- ---
title: ""ReNixing my OS""
tags:  nix, nixos, tutorial, nixpkg

---
-->

## My Tale of Woe

For most of my computing career, I've existed on a semi-random periodic cycle of OS reinstalls.  I'll inevitably download more projects, games, or apps, and fill up my primary SSD. Because SSDs have always been more expensive than spinning disks, and this process started when they first came out, I've had to do it a lot. 120/250/500GB is just not a lot of space anymore.

<img src=""https://benwis.imgix.net/too_much_stuff.jpeg"" alt=""too much stuff, not enough space"" style=""height: 20rem; margin: auto;""/>

In a previous cycle I learned about ZFS, an advanced filesystem with some very neat features such as easy snapshots, and easy transmission of filesystem volumes over the network. Sign me up! The peace of mind is amazing. Accidentally `rm -rf /` ? No problem! I can just restore the snapshot from an hour ago. Realize I deleted a file months ago that I now desperately need? Backups are available on the remote host. Everything was peachy.

That is, until this cycle came again, and I tried to replace my 500GB SSD with a 2TB one. I'm still a little fuzzy on the how, but my home directory volume got wiped. I can mount it, load the encryption key, but it was empty! And when I checked the backups, it had stopped uploading them months ago! D'oh!

<img src=""https://benwis.imgix.net/you_fool.jpeg"" alt=""The classic blunder, not checking your backups!"" style=""height: 20rem; margin: auto;""/>

Thankfully, my data volume was still quite alive and well, and as one does, I went to vent my frustration on Twitter. 
Twitter, Twitter on the wall, what's the fairest distro of all?

<blockquote class=""twitter-tweet""><p lang=""es"" dir=""ltr"">NixOS</p>&mdash; Sasha Savage (@TauOns) <a href=""https://twitter.com/TauOns/status/1573426345551745025?ref_src=twsrc%5Etfw"">September 23, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script>

Reading through that thread, I realized I was optimizing for the wrong thing. I had my data volume, which was good, but what if my root os could be stateless, and the instructions to create it saved? What if I had a fully declarative operating system, all rigorously defined and backed up? Enter NixOS.

## NixOS: An OS from a Config File

### Installation

Installation of NixOS was straightforward, roughly equal with any other graphical linux distro, albeit with fewer options for configuration in the installer itself. Initially the installer refused to run in the live environment, saying it was not connected to the internet. This was caused by the need to enter my WIFI details, but the GUI provided no way to do so.  Most distros would have some way to enter details graphically, or a note about how to connect. It was at this point that I found the [NixOS manual](https://nixos.org/manual/nixos/stable/), and it helpfully suggested I needed to run `nmtui` in the terminal. I got a bit farther in the Installer GUI before I bailed out. It was a bit basic. These days, I usually like to see different filesystems available for root, and an option to encrypt it. Thankfully it was very easy to bail out of the graphical installer and go to the terminal.

*I'm told the graphical installer is brand new in this release, so it's bound to have rough edges that'll improve once it's worked on!*

Because I had been running on a ZFS root, I decided to replicate that setup in NixOS, and the [documentation](https://nixos.wiki/wiki/ZFS) for that was painless. All I had to do was to get my SSD's device ID with `ls -l /dev/disk/by-id` and plug it in.

### Configuration

If there's one thing to know, its that almost everything in NixOS is controlled by your `configuration.nix` file in `/etc/nixos` and it's support files in that directory. By default, those are `configuration.nix` and `hardware-configuration.nix`. When you run through the install process, it'll generate a sample conf file with most of the options commented out, and a `hardware-configuration.nix` file. I do not recommend editing the hardware file, it is autogenerated.  You'll need to familiarize yourself with the available options,  enable the ones you want and then save the configuration file. Once saved, you actually regenerate your OS itself with `nixos-rebuild switch`. If you added new packages, that'll install them. If you enabled a setting or a service that'll start or restart them with your changes. It's both refreshing and drastically different than the experience in Ubuntu or Arch. It reminds me of the pre-systemd days in Arch, where a lot of things were set in your `rc.conf` but a lot more powerful.

By issuing `nixos-rebuild switch`, you're creating a [NixOS generation](), which is a snapshot of your system in that particular state. NixOS makes it easy to rollback to that configuration with  `nixos-rebuild switch --rollback`, and it adds that generation to your GRUB menu. If you happen to make a mistake editing your `configuration.nix` file and make your system unbootable, one can load the previous configuration in the Grub menu and revert the changes. One gotcha to note is that booting from an old config does not restore your old `configuration.nix` file or its support files, so if you screw those and you don't know how, you'll have to start again or hope they can be recreated with `nixos-generate-config`. I highly recommend you commit the files in `/etc/nixos` to git or keep copies in a safe place. Done properly, you can recreate your exact OS from these files and a NixOS live cd! 

There's very little that can't be done from `configuration.nix`. `You can enable wifi networks, touchpad drivers, sound, printing, a graphics environment and much, much more. The options seem endless, but common operations can be found in the [NixOS manual](https://nixos.org/manual/nixos/stable/). 

## Adaptation

If you happened to take a look at my original tweet, you might have noticed that I said I was getting tired of shenanigans. Let's be clear here, NixOS is not *""shenanigan free""*, but most of them are *""good shenanigans""*. 

<figure>
<img src=""https://benwis.imgix.net/i_swear_to_god.jpg"" alt=""I swear to god I'm going to pistol whip the next guy who says shenanigans!"" style=""margin: auto;""/>
<figcaption style=""text-align: center;"">Shenanigans *ducks*</figcaption>
</figure>	


It's a distro from a config file, with sane defaults, but needs to be optimized for best performance. And because some programs were not designed on or for it, you may have to go through extra hoops to get things installed and configured.

Take Visual Studio Code, for example. It has a package in the repository, but what about its plugins? Well, if it happens to be a package in the [NixOS repo](https://search.nixos.org/packages?channel=22.05&from=0&size=50&sort=relevance&type=packages&query=vscode-extensions), you can include it in the first array below.

```nix
(vscode-with-extensions.override {
         vscodeExtensions = with vscode-extensions; [
           bbenoist.nix
           ms-python.python
           ms-azuretools.vscode-docker
           ms-vscode-remote.remote-ssh
           github.vscode-pull-request-github
           editorconfig.editorconfig
           matklad.rust-analyzer
           mkhl.direnv
           jock.svg
           usernamehw.errorlens
           vadimcn.vscode-lldb
           bungcip.better-toml
           golang.go
           prisma.prisma
           jdinhlife.gruvbox
           ms-vscode.cpptools
           bierner.emojisense
           svelte.svelte-vscode
           jakebecker.elixir-ls
           denoland.vscode-deno
           graphql.vscode-graphql
           esbenp.prettier-vscode
           dbaeumer.vscode-eslint
           bierner.markdown-emoji
           _2gua.rainbow-brackets
           phoenixframework.phoenix
           mechatroner.rainbow-csv
           catppuccin.catppuccin-vsc
           brettm12345.nixfmt-vscode
           bradlc.vscode-tailwindcss
           shd101wyy.markdown-preview-enhanced
           ms-azuretools.vscode-docker
           justusadam.language-haskell
           xadillax.viml
           ] ++ pkgs.vscode-utils.extensionsFromVscodeMarketplace [
           {
             name = ""remote-ssh-edit"";
             publisher = ""ms-vscode-remote"";
             version = ""0.84.0"";
             sha256 =""33jHWC8K0TWJG54m6FqnYEotKqNxkcd/D14TFz6dgmc="";
           }
        ];
   })
```

*My VsCode configuration, look upon it and weep*

But if it's not in the nix repo, then it goes in the second array. Take remote-ssh-edit above. You need its package name(not the pretty shown one), publisher name, version, and sha256 hash. Most of that can be found on the [VsCode Marketplace](https://marketplace.visualstudio.com/vscode), all except for the hash. For that, you put in a wrong value, try to rebuild your config, and put in the hash  that it gives you when you throw an error. A similar thing occurs for neovim, my IDE of choice, but without the marketplace extensions.

Some things are too tricky for a simple NixOS package to install and configure correctly. A good example of this is Steam. One might think that it'd be easy to install, just add the steam package to your list of packages to install in `configuration.nix` like so:

```nix
environment.systemPackages = with pkgs; [
...
steam
...
];
```

For me, this errors out. It turns out that Steam needs [extra configuration](https://nixos.wiki/wiki/Steam). It needs a NixOS module. A NixOS module  is a file combined by NixOS to produce a system configuration. It has the ability to change parts of your configuration itself. It might use nixOS packages inside it, but it is fundamentally different. To use a module, all you need to do is enable it like so `programs.steam.enable = true;`, and potentially set some other options inside it

```nix
programs.steam = {
  enable = true;
    remotePlay.openFirewall = true; # Open ports in the firewall for Steam Remote Play
    dedicatedServer.openFirewall = true; # Open ports in the firewall for Source Dedicated Server
};
```

You would be forgiven for mixing those up. While you can't easily search for modules themselves on NixOS's website, you can [search for Options](https://search.nixos.org/options). Options are provided by Modules, and the providing module is included in the search results. For example, `programs.steam.enable = true` [is an option](https://search.nixos.org/options?channel=22.05&show=hardware.steam-hardware.enable&from=0&size=50&sort=relevance&type=packages&query=steam) provided by the steam module. If that option fails you, you can browse/search the [nixpkgs/modules repo](https://github.com/NixOS/nixpkgs/tree/master/nixos/modules), where modules are kept, and see if you can find it in there. Keeping this in mind, and reading the Wiki first, can help prevent some headaches.

### The Nix Language

I'd be remiss if I didn't mention the [Nix language](https://nixos.wiki/wiki/Overview_of_the_Nix_Language), which is used to write all of the `.nix` files and most of the Nix tools. It is a special language, used (exclusively?) for Nix and its Nix tools, and it is odd. Lists are space delineated, but attrSets aren't. Functions are created by assigning to a variable. Arguments are passed with spaces and are separated by colons. There's no marking around what are the function inputs and what is the function body. Functions are called by putting the function name followed by its arguments. No static typing or typing of any kind. No mutable state. Implicit returns.  And everything needs a semicolon. 100% functional.

```nix
      supportedSystems = [ ""x86_64-linux"" ""aarch64-linux"" ];
      whatIsThis = f: nixpkgs.lib.genAttrs supportedSystems (system: f system);
```

*Can you guess what this does?*

In this code block, supportedSystems is a new array that contains two strings. The next line defines the whatIsThis function, it takes `f` as an argument, and calls [nixpkgs.lib.genAttrs](https://github.com/NixOS/nixpkgs/blob/11c921f51aedd95ccbc3412e37d3ac6ea9d90e4a/lib/attrsets.nix#L413) with the `supportedSystems` array and a `(system: f system)` function (which takes a `system` argument and calls `f` with that argument). Did you get it right?

Coming from a Rust, Typescript, C, and Python background, this is uniquely difficult for me to parse. Admittedly I'm not a Haskeller, and I've never used Cue or Scheme, so it's probable that some people might have an easier time of this than me. I find myself wishing Rust was used for everything, or just that a language that is more C-like, but Nix predates Rust, so c'est la vie.

### Nomenclature

<blockquote class=""twitter-tweet""><p lang=""en"" dir=""ltr"">Stop calling everything <a href=""https://twitter.com/hashtag/Nix?src=hash&amp;ref_src=twsrc%5Etfw"">#Nix</a><a href=""https://t.co/0Habs0ReYo"">https://t.co/0Habs0ReYo</a></p>&mdash; Gabriella Gonzalez (@GabriellaG439) <a href=""https://twitter.com/GabriellaG439/status/1564241323871182848?ref_src=twsrc%5Etfw"">August 29, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script>

At this point, you could be forgiven if all the different Nix tools and things are starting to run together. Everything has Nix in the name, and it seems people aren't careful to distinguish them. As Gabriella Gonzalez helpfully points out, Nix is included in the name of a language, a package store, a packaging tool, an OS, and probably more. Check out their blog post for the details.  It's an ecosystem of things all somewhat confusingly named, that gives you superpowers. 

## Next Steps

Once you get all your packages installed, services configured, and settings set, you might think you're done. But you're not. Not really. For you have only scratched the surface of the potential of NixOS.

<figure>
<img src=""https://benwis.imgix.net/nixos_sickos.png"" alt=""The sickos meme, with Graham Christensen standing outside in the dark whose shirt says NixOS and they say Yes... HA HA HA... YES!"" style=""margin: auto;""/>
<figcaption style=""text-align: center;"">Graham Christensen approves, probably</figcaption>
</figure>																			


### Dev Shells

[Dev Shells](https://nixos.wiki/wiki/Development_environment_with_nix-shell) in NixOS are virtual environments, which should be familiar to you if you've ever used venv in Python. By adding a `shell.nix` file and defining the nix package dependencies, one can very easily create a virtual environment with all the packages and environment variables you need, simply by changing into the folder in a terminal and typing `nix-shell`. It makes it easy to have different versions of node or rust, python and its packages, set environment variables as needed, and provide a completely isolated and reproducible environment! 

#### direnv

By installing the `direnv` and `nix-direnv` packages, and doing [some config](https://github.com/nix-community/nix-direnv) in the project itself, one can even automatically load a virtual environment whenever you cd into that directory and unload it when you leave! No more fuss! I am in love with the efficiency of this feature! 

### Flakes

[Flakes](https://nixos.wiki/wiki/Flakes) are a new, unstable feature offering functionality similar to Rust's Cargo.toml and Cargo.lock mechanisms. Flakes can create a Nix package, define a Dev Shell, or even generate a complete NixOS system. They can be passed into nixos-rebuild to build the system with `nixos-rebuild switch --flake .`. Flake's `Flake.lock` mechanism ensures your dependencies are consistent, and not dependent on a git hash or the current channel package version. Flake based Dev Shells are faster than their predecessors as well, primarily by caching the nix environment evaluations, and they're natively supported in `direnv`! 

From project specific environments to creating packages in a standardized format to making system installation easier,  Flakes represent a major overhaul for the Nix ecosystem.  If you're tempted to use them, you should, but it's worth remembering that they are unstable and may change in upcoming releases. It certainly hasn't stopped me or many others from using them!

### Deployment

Because it is so easy to define a NixOS system, plenty of people use Nix to handle the configuration of multiple machines or servers. Tools like `deploy-rs` and `nix-deploy` make it easy to do that. One can even use it to build Amazon EC2 instances or Docker images!

### Home Manager

Once one gets comfortable with Nix, one can use [Home Manager](https://github.com/nix-community/home-manager) to manage the deployment of all the config files in your home directory that aren't being handled by NixOS already. Home Manager also has some packages that further declaratively define your environment. It's worth checking out, but it's warned that it's not for the inexperienced NixOS user due to the possibility of weird errors and the need for manual intervention. 

## Conclusion

 I didn't expect to like NixOS quite as much as I did. I think it's going to remain my Linux distro of choice for a while. For me, tools(and distros) usually fall into one of four categories.

1. Easy to learn but limited
2. Easy to learn, and powerful
3. Hard to learn, and bad
4. Hard to learn, but worth it

Here it is in handy graph form.

<img src=""https://benwis.imgix.net/learning_curve_power.png"" alt=""A four quadrant graph. X axis is labeled power and Y axis is labeled Learning Curve. Rust is highest in both power and learning curve, NixOS is a bit less in power and learning curve, Typescript is a bit less in power and learning curve. NixLang is in quadrant II, meaning it is lower in power but still fairly high in Learning Curve. Python is in Quadrant IV, making it low in learning curve but still fairly high in power"" style=""height: 20rem; margin: auto;""/>

NixOS falls solidly into Quadrant I, where things go that are hard to learn but offer real benefits. 

### Should One Use NixOS and Nix?

It's not for the faint of heart, it requires solid knowledge of Linux fundamentals, learning at least some of an oddly different language to configure it, and adapting your imperatively defined packages into declaratively packaged ones. 

I am not sure I would have stuck with it if not for the help of @a_hoverbear, @jakehamiltondev, and @TauOns on Twitter. Huge thanks to them for answering all my questions! It ended up taking about 3 days, but I now have a working NixOS Workstation, with VS Code and Neovim installed, and all the necessary compilers for Rust and Typescript. And plenty of things I could upgrade and change to make things even more powerful.

It's a solid distro for a workstation or server. The benefits of easy reproducibility, stability, seamless creation of virtual dev environments, and simple rollbacks make it a powerful tool that I hope to learn better in the months ahead. If you're a seasoned Linux user and are tempted by the benefits, I'd say it's well worth picking it up and give it a whirl. ",,renixing-my-os,1,1,"",,,1662076800,1680129650
8,1,Better Web Apps on the Desktop,"The last couple decades have seen a trend, a trend away from native desktop apps, towards web and mobile apps. Then Electron came along, and made web apps run on the desktop. But the apps were slow and clunky, perhaps there's a better way.","<!-- 
---
title: ""Better Web Apps on the Desktop""
tags:  web, native app, desktop app, rust, egui, eframe, tauri
---
-->

The last couple decades have seen a trend, a trend away from native desktop apps, towards web and mobile apps. Developers know that building for the web can net them the eyeballs of every computer user, and they have to build mobile native because phone companies have systemically repressed web apps in favor of native ones. Remember Steve Jobs' proclaimed that ""You can create amazing Web 2.0 apps that look exactly like native apps on the iPhone"" at WWDC 2007? That certainly didn't come to pass, maybe because it would have prevented them from taking a [large cut](https://www.theverge.com/21445923/platform-fees-apps-games-business-marketplace-apple-google) of In App Purchases. Difficult to say...

Then in 2013, Github built Atom Shell, a framework that allowed them to transform a web app into a cross platform desktop app. While developed for Atom, their hackable open source text editor, they realized the potential of it as a framework for any app. And so, in 2014, Atom Shell was renamed to Electron, and open sourced under the MIT license. A couple years later, in 2016, Electron hit version 1, and apps built with it became submittable to the Mac and Windows App store later that year. And the web apps on the desktop took off.

<figure>
<img style=""height: 30rem; margin: auto;"" src=""https://benwis.imgix.net/atom_screenshot.png"" alt=""Atom Text Editor"">
<figcaption style=""text-align: center;"">I always preferred Sublime Text, it was always so much snappier</figcaption>
</figure>



Big names like Discord, Slack, Microsoft Teams, Spotify, Zoom, Visual Studio Code, and tons of others all bought in and built desktop apps with it. Since desktop apps were now essentially regular web apps, it allowed web developers to design diverse cross platform interfaces with what they already knew. But it wasn't without its downsides. Electron packages the full Chromium browser, a Node Server, and the V8 Javascript engine inside itself. This means that Electron apps eat far more than their fair share of memory, cpu, battery, and disk space. Running more than one or two Electron apps could bog down your computer, and it encouraged apps to require internet access to work. The rise of Electron (and React Native) solidified HTML/CSS and Javascript as the most coveted skills for developers across the web, desktop, and mobile spheres.  

Electron's flaws did not go unnoticed by all, and now we have some new contenders for GUI frameworks that can run on the web and desktop. 

## New Options

### Tauri

Around [July 2019](https://twitter.com/parker_codes/status/1588987254047145985), Tauri was created, promising to provide an ""optimized, secure, and frontend-independent application for multi-platform deployment"". Following in the footsteps of electron, it continues to allow the use of HTML, CSS, and JS frameworks for frontend design, but replaced the Chromium browser and Node server with their own Rust backend. By doing so, they've improved security, massively reduced memory and cpu usage, and remain cross platform. Today one can use any number of JS web frameworks like Svelte, Vite, Next.js, or vanilla JS or even some Rust frameworks like egui or Yew to render the frontend, giving unparalleled choice for developers.

Since no server is bundled in, it is required to build either a Single Page App(an app that downloads itself as files and runs on your computer) or a statically generated app(one that is rendered to the server and served as static files). Unfortunately, using Server Side Rendering would require the addition of a server to the bundle.

### Egui

One of the leading Rust GUI frameworks, [Egui](https://github.com/emilk/egui), is a simple, fast, and highly portable immediate mode GUI library for Rust apps. What's an immediate mode GUI you ask? Let's take a look at the egui docs.

> In a retained GUI you create a button, add it to some UI and install some on-click handler (callback). The button is retained in the UI, and to change the text on it you need to store some sort of reference to it. By contrast, in immediate mode you show the button and interact with it immediately, and you do so every frame (e.g. 60 times per second). This means there is no need for any on-click handler, nor to store any reference to it. Egui has seen plenty of integration with Rust game engines, allowing for the creation of in-game menus and interfaces. Because it is built with Rust, it can be compiled into both a native app, and directly to Webassembly, where it can be run in the browser. -egui Docs

Immediate mode GUIs are simpler to write, simpler to render, and eliminate entire classes of bugs related to state management and event handling of retained mode GUIs like Qt. But they do take a small performance hit since the GUI needs to be rerendered after each frame(or mouse movement/interaction).

Egui also leverages Rust's ability to compile to both a native app and to Wwebassembly, allowing one to run your app in any web browser! 

As a sidenote, one of the common refrains I hear is that Webassembly has no native way to interact with the DOM, and thus suffers a large performance penalty when it has to call out to Javascript to do so. While that is [debatable](https://crates.io/crates/sledgehammer) as a general rule, it is certainly true for most Webassembly based web frameworks right now. [Benchmarks](https://krausest.github.io/js-framework-benchmark/index.html) put Webassembly based frameworks behind vanilla JS, and most fast JS frameworks. It's still faster than React though!

The neat thing about Egui is that absolutely none of that applies. Egui uses WebGL directly, so there is no DOM at all. Everything is drawn to a [Canvas](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API) and displayed by the GPU. While this certainly has some downsides in accessibility and observability, it completely obviates that pesky layer crossing cost. The only remaining major downside for WASM apps are the fairly large bundle size.

### Why these two?

I'm specifically calling out these two GUI frameworks, because they both allow you to achieve what Electron can. They  both can deploy a web app and a desktop app from the same codebase. So that when you can build the next Discord, or Zoom, or Spotify, you can avoid those nasty performance hurdles and leverage the power of Rust! Let's try them out!

## Project Setup

### Tauri

After reaching out on Twitter, the lovely folks at Tauri(shoutout to [@parker_codes](https://twitter.com/parker_codes) for answering my questions and [@lucasfernog](https://github.com/lucasfernog) for creating the Tauri example) created the [web example](https://github.com/tauri-apps/tauri/tree/dev/examples/web), which shows how to create both a web app and a desktop app from the same repo with Svelte. Let's set it up!

```bash
git clone ""git@github.com:tauri-apps/tauri.git""
.scripts/setup.sh # This step needs to be run in the repo root, and is missing from the README
cd examples/web
yarn # or npm install
```

And that's it! One can run the desktop app with `yarn tauri dev` if you installed the NPM CLI and `cargo tauri dev` if you didn't, and the web app with `yarn dev:web`.
This works through the power of conditional compilation in your vite.config.ts folder

```typescript
import { resolve } from 'path'
import { sveltekit } from '@sveltejs/kit/vite'
import wasm from 'vite-plugin-wasm'
import topLevelAwait from 'vite-plugin-top-level-await'
import { viteStaticCopy } from 'vite-plugin-static-copy'
import type { UserConfig } from 'vite'

const TARGET = process.env.TARGET

const plugins = [sveltekit()]

if (TARGET === 'web') {
  plugins.push(wasm())
  plugins.push(topLevelAwait())
  plugins.push(
    viteStaticCopy({
      targets: [
        {
          src: 'core/wasm/pkg/wasm_bg.wasm',
          dest: 'wasm'
        }
      ]
    })
  )
}

const config: UserConfig = {
  plugins,
  resolve: {
    alias: {
      $api:
        TARGET === 'tauri'
          ? resolve('./src/api/desktop')
          : resolve('./src/api/web')
    }
  }
}

export default config
```

As we can see, if the `TARGET` env variable is WEB, it'll copy the wasm into the build, and resolve the `$api` alias to either the desktop or web bridge code that'll call the Rust backend. The desktop folder calls the Rust backend directly with Tauri, and the web folder calls the rust code compiled to Webassembly. Exciting! Unlike the egui example, this example does use the DOM and does suffer some performance penalties. However, it follows a more traditional web development approach, and can benefit from the accessibility improvements that browsers feature. 

> EDIT:My PR to fix this has been merged. I'm leaving it here for reference struck through. 

<del>However, there are two bugs in the example repo. First, the vite config requires you to whitelist the WASM file so it will load for your web app. You should get an error if you try to run it without this, but I didn't always gets one, which was somewhat frustrating. This can be done by modifying the above UserConfig section like so:</del>

```typescript
const config: UserConfig = {
  server: {
    fs: {
      // Allow serving files the project root
      allow: ['.']
    }
  },
...
}
```

Once that's patched, one can generate the web version using `yarn dev:web`.

<figure>
<img style=""height: 20rem; margin: auto;"" src=""https://benwis.imgix.net/tauri_web.png"" alt=""Web version of the Tauri Example Greet App"">
<figcaption style=""text-align: center;"">Hitting Greet will open an alert box saying, ""Hello, Tauri!</figcaption>
</figure>

<del>For mac at least, the Desktop version has its own bug. If you run `cargo tauri dev` and it just keeps waiting for the server to start, you might need to edit the `devPath` in `examples/web/core/tauri/tauri.conf.json` and replace `127.0.0.1` with `localhost`. On my mac, this was the only way to get it to the dev command to run successfully.</del>

The desktop version can be run with `cargo tauri dev`, and produces an output like so: 

<figure>
<img style=""height: 30rem; margin: auto;"" src=""https://benwis.imgix.net/tauri_desktop.png"" alt=""Desktop version of the Tauri Example Greet App"">
<figcaption style=""text-align: center;"">Look Ma, a Native App!</figcaption>
</figure>


After that, it should be smooth sailing!

### Egui and Eframe

As mentioned earlier, Egui can be compiled to a native Rust app or to a WASM app through the power of Rust! [Eframe(https://github.com/emilk/egui/tree/master/crates/eframe) is the official framework library for writing apps using egui, and it so happens to feature a [handy template](https://github.com/emilk/eframe_template/) that makes creating a dual purpose egui app very easy. Let's dive in!

Firstly, one should have Rust installed, which you can check by running:

```bash
rustup update #Check that the latest rust is installed
```

Then, if you're on an Ubuntu linux clone you need to install these deps

```bash
sudo apt-get install libxcb-render0-dev libxcb-shape0-dev libxcb-xfixes0-dev libspeechd-dev libxkbcommon-dev libssl-dev
```

or a Fedora based one:

```bash
dnf install clang clang-devel clang-tools-extra speech-dispatcher-devel libxkbcommon-devel pkg-config openssl-devel libxcb-devel fontconfig-devel
```

Than you create your own version of the eframe template repo, then clone it:

```bash
git clone https://github.com/benwis/eframe_template
cd eframe_template
```

Then it's as simple as running `cargo run --release` for a local version. 

<figure>
<img style=""height: 30rem; margin: auto;"" src=""https://benwis.imgix.net/egui_desktop.png"" alt=""The Eframe Template Desktop app"">
<figcaption style=""text-align: center;"">Success!</figcaption>
</figure>



To get the web version building, we need to install trunk, the Rust web server:

```bash
cargo install --locked trunk
```

Then generate the web version with 

```bash
trunk serve
```

which will build the site and serve it on `http://127.0.0.1/index.html#dev`. By appending #dev to the url, we prevent assets/sw.js from trying to serve us the cached version. Once loaded, even the web version should work fully offline! Like the Tauri version, Egui is compiled to WASM for the Web and then downloaded as static files, and directly built and called when running as an app.

<figure>
<img style=""height: 30rem; margin: auto;"" src=""https://benwis.imgix.net/egui_web.png"" alt=""The Eframe Web app, rendered from the same codebase!"">
<figcaption style=""text-align: center;"">I know I have a lot of tabs, this isn't even my final form!</figcaption>
</figure>



Check out the more fully featured online demo [here](https://www.egui.rs/#demo) and experience the wonder. It's got floating tabs, foreground and background, and so much more. It's totally different from the traditional web interface conventions we've come to expect!

<figure>
<img style=""height: 30rem; margin: auto;"" src=""https://benwis.imgix.net/egui_full_demo.png"" alt=""TThe fully featured egui demo page. Experience the overlapping tabs and functionality"">
<figcaption style=""text-align: center;"">All directly rendered by WebGL and your GPU!</figcaption>
</figure>

## Conclusion

Whether you're looking to redeploy your Electron app into a more performant version, or just want to give your users a better desktop app experience, Rust has at least two viable options to consider. It's nice to be able to provide that without sacrificing your users' processing power. I look forward to seeing how Tauri and egui develop in this space, and to all the exciting things that can be built using them. If you have any questions or have built something interesting, feel free to msg me on Twitter or Mastodon at the links in the footer. Happy All Hallows Day!",,better-desktop-web-apps,1,1,"",,,1664928000,1680129562
9,1,Bridging the Server<|_|>Client Divide,"If you've been paying attention to Web Dev Twitter, it'd be hard to miss the increasing number of tweets decrying the choice of GraphQL for their applications. and how much better tRPC or their framework is. Let's dig into that.","<!--
---
title: ""Bridging the Server<|_|>Client Divide""
tags:  leptos, isomorphic, typesafety, trpc, graphql, remix, solid, nextjs
---
-->

If you've been paying attention to Web Dev Twitter, it'd be hard to miss the increasing number of tweets decrying the choice of GraphQL for their applications. and how much better tRPC or their framework is. Let's dig into that.

<blockquote class=""twitter-tweet tw-align-center""><p lang=""en"" dir=""ltr"">‚ÄúMost apps don‚Äôt  benefit from [GraphQL].‚Äù <a  href=""https://t.co/WVyvrRPSpp"">https://t.co/WVyvrRPSpp</a></p>‚Äî ‚ú® Josh Branchaud ü¶É (@jbrancha) <a  href=""https://twitter.com/jbrancha/status/1597075160938336257?ref_src=twsrc%5Etfw"">November 28, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js""  charset=""utf-8""></script> 

While some of this is definitely hyperbolic, it can be distilled down to a much simpler problem. How do developers most easily connect their frontend code, which is responsible for displaying and collecting data, and the backend, which stores and manipulates that. GraphQL was not designed to be that!

GraphQL was designed to be a type safe declarative data collection schema language. Facebook made it to collect only the data they needed from their myriad network of microservices flexibly. It's supposed to insert itself into your data pipeline, and provide type guarantees from a loosely connected group of interrelated services. It has an entire type system that needs to be converted into and out ofto the destination language of choice. Using it solely for it's type safety guarantees is missing the forest for the trees. [Roy Derks](https://hackteam.io/blog/compare-graphql-and-trpc) does a great job of laying out the differences, so check that out if you want more detail.

## Solutions

What should tRPC be compared with? Well, I think it's fair game to compare it against other solutions to the server-client problem across the web ecosystem.  Let's look at a few of them:

### API Frameworks

#### tRPC

tRPC bills itself as the solution to providing end-to-end typesafe APIs for Typescript in React, and it's hard to argue with that. A change made to a server function signature will produce a Typescript error in the client, and vice versa. tRPC is heavily tied to Typescript, so if you use another language, you are out of luck. It definitely has it's proponents, notably Theo of Ping.gg.

<blockquote class=""twitter-tweet tw-align-center""><p lang=""en"" dir=""ltr"">The amount that tRPC has improved the quality of our code, the speed of our delivery,  and the happiness of our devs is hard to  comprehend.<br><br>I know I shill it a lot but seriously,  please try <a  href=""https://twitter.com/trpcio?ref_src=twsrc%5Etfw"">@trpcio</a></p>‚Äî Theo - ping.gg (@t3dotgg) <a  href=""https://twitter.com/t3dotgg/status/1571922456239284224?ref_src=twsrc%5Etfw"">September 19, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js""  charset=""utf-8""></script>

tRPC is designed to provide exportable types from the server, and an interface to call procedures(functions) from the client. Because it doesn't abstract away the serialization and transport layer completely, it also recommends Zod to validate the client inputs, requiring a bit more work to implemnt. Let's see how one would implement this, straight from the [tRPC docs for Next.js](https://trpc.io/docs/nextjs).

Skipping install steps, we need to create a router to process the incoming API calls. 

```typescript
// server/tprc.ts
// This creates the router instance along with its helpers
import { TRPCError, initTRPC } from '@trpc/server';
// Avoid exporting the entire t-object
// since it's not very descriptive.
// For instance, the use of a t variable
// is common in i18n libraries.
const t = initTRPC.create();
// Base router and procedure helpers
export const router = t.router;
export const procedure = t.procedure;
```

Then the router needs to be imported and constructed in your `server/routers/_app.ts file`

```typescript
// server/routers/_app.ts
import { z } from 'zod';
import { procedure, router } from '../trpc';
export const appRouter = router({
  hello: procedure
    .input(
      z.object({
        text: z.string(),
      }),
    )
  // You'd want to break the functions out into separate files in a prod app
    .query(({ input }) => {
      return {
        greeting: `hello ${input.text}`,
      };
    }),
});
// export type definition of API
export type AppRouter = typeof appRouter;
```

Then the trpc routes need to be registered as a wildcard route.

```typescript
// pages/api/trpc/[trpc].ts
import * as trpcNext from '@trpc/server/adapters/next';
import { appRouter } from '../../../server/routers/_app';
// export API handler
export default trpcNext.createNextApiHandler({
  router: appRouter,
  createContext: () => ({}),
});
```

Export the global tRPC object as a global from the app root.

```typescript
// pages/_app.tsx
import type { AppType } from 'next/app';
import { trpc } from '../utils/trpc';
const MyApp: AppType = ({ Component, pageProps }) => {
  return <Component {...pageProps} />;
};
export default trpc.withTRPC(MyApp);
```

Finally, it can be called from your React component.

```typescript
// pages/index.tsx
import { trpc } from '../utils/trpc';
export default function IndexPage() {
  const hello = trpc.hello.useQuery({ text: 'client' });
  if (!hello.data) {
    return <div>Loading...</div>;
  }
  return (
    <div>
      <p>{hello.data.greeting}</p>
    </div>
  );
}
```

Admittedly, this is the stack I am least familar with, but this also seems to be the most clunky setup of the options we'll discuss. Most of the RPC approaches we'll discuss externalize most of the work to a compiler through the use of macros or custom bundler plugins. It's hard for me to get excited about the need to both define a function and input validation in the router, and then to call it using a property of a property in an object. I know your VSCode Intellisense will help you get everything right, it's just a lot of boilerplate. 

Not to say that tRPC isn't the best approach for React/Next.js apps, I haven't done nearly enough with it to be sure. It does have a very nice integration with Tanstack Query, and the type safety is quite real. I'm just not in love with the API. If you'd like to check out tRPC, one good way is to use the [t3 Stack](https://create.t3.gg/), Theo's starter stack for Next.js and tRPC. 

#### gRPC

If multi-language support is a requirement, gRPC/gRPC-web definitely can pull its weight. It provides a schema to enforce typesafety across a variety of languages, has more types than GraphQL, can encode to binary, can handle streaming*, and is much faster than REST. It's missing a decent Typescript server, but if your backend runs in Go, Rust, or another language with one, then the lovely folks at Buf have created a Typescript library called `connect-web` [here](https://github.com/bufbuild/connect-web) that will automatically generate Typescript bindings from the schema.

gRPC is not a closely coupled solution between a client and a server. In that way, it's better to compare it to GraphQL than tRPC. It requires a lot of the same intermediate steps that GraphQL does. Types must be generated from a `protobuf` file, and then imported in your web client. I'd reach for this if the data being transferred between frontend and backend is primarily binary(like images and video), or if you need streaming. gRPC is much stronger communicating between servers or between servers and mobile/desktop apps, than it is between servers and the browser due to some of the limitations of gRPC-web. If you're interested in learning more about gRPC, I wrote about it a short time ago [here](https://benw.is/posts/the-gist-of-grpc).

```typescript
const answer = await eliza.say({sentence: ""I feel happy.""});
console.log(answer);
// {sentence: 'When you feel happy, what do you do?'}
```

*connect-web has a pretty nice UI though*

## Framework Solutions

### Remix

I owe Remix a debt, in that they introduced me to the idea of the client-server gap in the first place. As Ryan Florence proclaimed on Twitter in Jan 2022, Remix is a ""center stack"" framework. Remix's core argument is in essence

>  API framework? The browser has everything we need right here.

<blockquote class=""twitter-tweet tw-align-center""><p lang=""en"" dir=""ltr"">Forget ‚Äúfull  stack‚Äù. Remix is center stack.<br><br>It‚Äôs an HTTP tunnel  between interactivity and business logic. It‚Äôs the part of the stack  that no tool before it has solved, not completely.<br><br>I  think I finally know how to talk about Remix.</p>‚Äî Ryan  Florence (@ryanflorence) <a  href=""https://twitter.com/ryanflorence/status/1482233495195770883?ref_src=twsrc%5Etfw"">January 15, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js""  charset=""utf-8""></script> 

Utilizing a custom Typescript/Javascript bundler built on esbuild, the team made it possible to colocate the server code and the client code in the same file. Querying data is as simple as sending a GET Request to a particular endpoint, which runs a `loader` function and returns a Response. Data mutations are POST Requests to an endpoint, which runs the `action` function. Action inputs are typically form submissions, encoded with `application/x-www-form-urlencoded` and Responses are JSON objects that can be handled by the loader and passed to your React components with handy utility functions. End to end typesafey can be inferred from the Response types included in the `loader` and the `action`.

```typescript
import type { LoaderFunction, ActionFunction } from ""@remix-run/node"";
import { json } from ""@remix-run/node"";
import { Form, useActionData, useLoaderData } from ""@remix-run/react"";

// Run on a GET request to the route the file represents
export const action: ActionFunction = async ({ request }) => {
  const formData = await request.formData();
	let firstName = formData.get(""firstName"");
  if (!firstName){
    return json({
      error: {
        firstName: ""Name not found!""
      }
    })
  }
  
  // store the data
  await setName(firstName);
  
  // returns json
  return json({
    firstName,
  });
};

// Run on a GET request to the route the file represents
export const loader: LoaderFunction = async ({ request }) => {
	let firstName = await fetch(""https://example.com"");
  		// returns json
      return json({
        firstName,
      });
  }
};
export default function Index() {
  const { firstName } = useLoaderData();
  ...
}
```

This heavy use of the Request/Response framework, colocation of server and client functions, and heavy use of the browser have set Remix on a remarkable growth curve. It's not all roses though. Because of React's structure, there's no easy way for Remix/React so selectively update one thing in a component/page/layout. Submitting a form would hit the `action` function and refresh all the data covered by that `loader`. Some of that can be mitigated through the use of nested layouts, but it's impossible to completely eliminate.

### Solid and Solid Start

SolidJs is the OG reactive framework for Javascript/Typescript. Ryan Carniato and the gang have created an entirely different method of rendering web pages, without the use of the VDOM. The reactive framework has a lot of benefits, most notably increased performance, and fine grained control of data fetching and rerendering. To learn more about that, check out [this video](https://youtu.be/qB5jK-KeXOs) by Dan Jutan introducing it, and Ryan's blog [post](https://dev.to/ryansolid/a-hands-on-introduction-to-fine-grained-reactivity-3ndf) on the basics. 

Because we have fine grained control of rendering, it lets us selectively update DOM elements whenever data changes.   Each Signal, or piece of data, keeps track of which Effects depend on it. If it updates, it will rerun only those Effects and only the DOM elements that need to be updated.  This allows us to be more fine grained with our updates, and define a more granular API. One way Solid does this is with `server$`. The [docs](https://start.solidjs.com/api/server) give a good description of how it works, I'll borrow some of their code below:

```typescript
import server$ from 'solid-start/server'
¬†
function Component() {
  // This will only console.log() on the server
  const logHello = server$(async (message: string) => {
    console.log(message)
  });
¬†
  logHello('Hello')
}
```

Here we define a `logHello` function that takes in a `message` string and  `console.log()`s it. The interesting part is the `server$()` method, which is compiled and replaced by the Solid compiler. On the client, it subs out the call for a data fetching method. On the server, it adds a data decoding method, and the regular function code.

```typescript
// Server code
import server$ from 'solid-start/server'
¬†
// COMPILATION OUTPUT on the server
server$.registerHandler(
  '/Log.tsx/logHello', 
  async (message: string) => {
    console.log(message)
  }
)
const serverFunction1 = server$.createHandler('/Log.tsx/logHello', '#')
¬†
function Component() {
  const logHello = serverFunction1;
¬†
  logHello('Hello')
}
// Client code
import server$ from 'solid-start/server'
¬†
// COMPILATION OUTPUT on the client
const serverFunction1 = server$.createFetcher('/Log.tsx/logHello')
¬†
function Component() {
  const logHello = serverFunction1;
  
  logHello('Hello')
}
```

This is all enabled by the Solid compiler, which generates both the server and client versions. The client call to `logHello()` is replaced with a `createFetcher('/Log.tsx/logHello', '#')`, whose job it is to encode the arguments and then send those as a POST request to the handler the compiler created for this function. The server version registers the route handler, passes the input args to the function, and serializes the output to be sent to the client(if there were any).

I really love this model. We're able to define the function call in the client, we've got end to end typesafety because it's all generated from one typescript function, and it's colocated with the relevant client logic. If we were so inclined, we could tie this function to a Signal and get a surgical update. 

### Leptos

I've been watching the Rust frontend framework community with baited breath, looking for something that competes with or beats React. A couple months ago I found Leptos, the most promising Rust framework I've seen yet. It is heavily based on Solid, and beats React in every [benchmark](https://user-images.githubusercontent.com/286622/198388168-d21e938b-5d59-4000-b373-91b48f1ec4d3.png), except for bundle size.

Greg Johnston, Leptos' founder, also implemented his own version of Solid's Server Functions. When asked about his inspiration for the feature, he said it was heavily influenced by Solid and Remix. We'll talk about how in a bit. To use a server function, we need to define a function whose args and `T` Response type implement `serde::Serialize` and `serde::Deserialize`. Functions are processed in Leptos using the `server` proc macro, which does a similar thing to Solid's compiler above. The macro allows you to specify a function name, the route to place the function at, and the encoding used for serializing and deserializing the results.

```rust
#[server(AddTodo, ""/api"", ""Url"")]
pub async fn add_todo(title: String) -> Result<(), ServerFnError> {
    let mut conn = db().await?;

    // fake API delay
    std::thread::sleep(std::time::Duration::from_millis(1250));

    match sqlx::query(""INSERT INTO todos (title, completed) VALUES ($1, false)"")
        .bind(title)
        .execute(&mut conn)
        .await
    {
        Ok(row) => Ok(()),
        Err(e) => Err(ServerFnError::ServerError(e.to_string())),
    }
}
```

Inspired by Remix, the default encoding for args when sent to the server is `application/x-www-form-urlencoded` and server results are returned as JSON. This makes it easy to use these to process form submissions and return results. If the data involves some large binary objects, or the overhead of serialization/deserialization into JSON is too much, you can choose to use `""Cbor""` encoding instead, which should net you some nice performance gains.

Next we have to register our server functions. Unlike JS, Rust won't let us randomly mutate our global objects, we need to have an explicit registration step to setup the route handlers for the server functions.

```rust
// Define a function to register them(useful if you have more than one)
pub fn register_server_functions() {
            AddTodo::register();
        }
// Call the function inside our main function
 crate::todo::register_server_functions();

```
If you haven't already, you'll need to make sure the router knows how how to handle the POST requests that are sent to the server. To do that, we add a wildcard route to our backend server.
```rust
/// Other routes omitted for brevity. This is for Axum, Actix would be a bit different.
let app = Router::new()
        .route(""/api/*fn_name"", post(leptos_axum::handle_server_fns));
```
Then we can call it in our client code as if it was defined locally.

```rust
// Creates an Action to synchronize an imperative async function call to the synchronous reactive system.
let add_todo = create_server_multi_action::<AddTodo>(cx);

view! {
        cx,
        <div>
  			//MultiActionForm automates the form handler and the event handlers
            <MultiActionForm action=add_todo>
                <label>
                    ""Add a Todo""
                    <input type=""text"" name=""title""/>
                </label>
                <input type=""submit"" value=""Add""/>
            </MultiActionForm>
            <div>
}
```

Much like Solid's compiler, the `server` proc_macro makes this possible. Like Solid and tRPC, we get end to end typesafety because it's all generated from one function declaration. There's a bit more boilerplate in the Rust version due to the more stringent borrow checker and ownership semantics, but I don't have to worry about input validation or serialization/deserialization. If you'd like to look at the full example these code samples were derived from, it can be found [here](https://github.com/gbj/leptos/tree/main/examples/todo-app-sqlite-axum) in the leptos repo. 

## Conclusion

Which solution you choose is going to depend heavily on what your needs are and what stack you'd like to use. If it's a greenfield project on a smaller team, Remix or Solid might enable you to build with less boilerplate. If you're a Rustacean like me, Leptos has you covered. And if you're tied into the React/Next.js ecosystem, tRPC might be the library for you.

If you're choosing a solution for a distributed API, one that needs to get data from multiple sources, or your frontend and backend teams are entirely separate, GraphQL or gRPC is a better choice than any RPC framework. They provides an enforcable, versioned schema that is understood by a variety of services and servers in many different languages. 

Quit it with the tRPC v GraphQL posts. All that says to me is that you didn't understand the needs of your web app when you built it, or that you built it before tRPC was a thing. It's not a fair comparison.

In a lot of ways, I think the popularity of tRPC comes from solving a problem inherent to Next.js and React. If Next.js were to implement similar RPC functionality as Solid, Leptos, or Remix, I don't see it surviving long term. I found this tweet after I wrote this piece, but it sums things up very nicely.

<blockquote class=""twitter-tweet tw-align-center""><p lang=""en"" dir=""ltr"">Reason why I say that is cause I see frameworks starting to build in native typesafe RPC  functionality.<br><br>But I don't think they will  build in GraphQL functionality and straight up RPC calls don't  entirely replace what you get from GraphQL.</p>‚Äî ‚òÉÔ∏è  Anthony (ajcwebdev.x) ‚ùÑÔ∏è (@ajcwebdev) <a  href=""https://twitter.com/ajcwebdev/status/1597335794414678016?ref_src=twsrc%5Etfw"">November 28, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js""  charset=""utf-8""></script> 





",,bridging-the-divide,1,1,"",,,1667001600,1680214049
10,1,Webassembly in Remix,"Today is a momentous day for me, a culmination of nearly seven months of waiting. My pull request adding Webassembly(often abbreviated as WASM) support to Remix has finally been merged! But how we do we use it?","<!--
---
title: ""WebAssembly in Remix!""
tags:  WebAssembly, Remix, Rust, wasm
---
-->
## Intro

Today is a momentous day for me, a culmination of nearly seven months of waiting. My [pull request](https://github.com/remix-run/remix/pull/3299#issuecomment-1310979410) adding Webassembly(often abbreviated as WASM) support to Remix has finally been merged! While not in an official release yet, I thought it would be interesting to revisit some of the neat things that can be done with WebAssembly in Remix that I talked at RemixConf way back in May this year.

But first, let's review how Webassembly in Remix can be used. A typical WASM bundle typically consists of two things, a binary`.wasm`  file that houses the acutal compiled code, and a JS file that contains a number of autogenerated helper functions. These functions will convert function inputs and outputs from other types into i32,i64,f32, and f64 or v128, do other administrative tasks, and an init() function that is used to load the WASM file into memory. Above that, the rest is packaging.

The trouble comes getting the binary `.wasm` files through the Remix compiler and into the output bundle. The PR I submitted essentially tells esbuild that if it finds an import for a `.wasm` file, it should be treated as a static file, and dropped into the `public/build/_assets` folder as part of the bundle output. This works great for WebAssembly files that you control, and for npm packages that offer you flexibility, but becomes a bit dicier when dealing with packages that make assumptions about the end use environment.  As is often the case, the JS ecosystem is squarely to blame here, making things harder then they need to be.

## Loading Webassembly in JS Environments

In the end, the trouble comes down to how Webassembly files are instantiated. Before a function in webassembly can be called, it needs to be loaded into memory and analyzed by the JS environment running it. Because there is no common standard, and each environment was developed separately by different people at different times, implementations vary.

My favorite implementation comes from Node, which added [support](https://nodejs.dev/en/learn/nodejs-with-webassembly/) for Webassembly in Node v8. A sample from their docs can be seen below. The wasmFile is loaded into memory, instantiated, and then we can call into it!

```javascript
// Assume add.wasm file exists that contains a single function adding 2 provided arguments
const fs = require('fs');

const wasmBuffer = fs.readFileSync('/path/to/add.wasm');
WebAssembly.instantiate(wasmBuffer).then(wasmModule => {
  // Exported function live under instance.exports
  const { add } = wasmModule.instance.exports;
  const sum = add(5, 6);
  console.log(sum); // Outputs: 11
});
```

Here's a sample from my [remix-rust](https://github.com/benwis/rust-remix) repo, in the rust_functions package. It gets generated when `wasm-pack build --target node` is run.

```javascript
/* @param {number} a
* @param {number} b
* @returns {number}
*/
module.exports.add = function(a, b) {
    const ret = wasm.add(a, b);
    return ret >>> 0;
};

...
const bytes = require('fs').readFileSync(path);

const wasmModule = new WebAssembly.Module(bytes);
const wasmInstance = new WebAssembly.Instance(wasmModule, imports);
wasm = wasmInstance.exports;
module.exports.__wasm = wasm;
```

This also makes it pretty easy to call it, as I do in the example repo, from within Node in one of Remix's Action or Loader functions.

```javascript
import { add } from ""rust_functions"";

export const action: ActionFunction = async ({ request }) => {
    const result = add(2,2);
      console.log(""result"", result);
      return json({
        result,
      });
};

```

Seems pretty straightforward right? Import and use.

Unforunately the Webassembly story inside the browser isn't quite as rosy. While supported, there's no integration in the browser between script type or import statements, so we need to manually load the `.wasm` file. Check out this code generated by `wasm-pack --target web`, which is designed to be loaded in the browser.

```javascript
async function load(module, imports) {
    if (typeof Response === 'function' && module instanceof Response) {
        if (typeof WebAssembly.instantiateStreaming === 'function') {
            try {
                return await WebAssembly.instantiateStreaming(module, imports);

            } catch (e) {
                if (module.headers.get('Content-Type') != 'application/wasm') {
                    console.warn(""`WebAssembly.instantiateStreaming` failed because your server does not serve wasm with `application/wasm` MIME type. Falling back to `WebAssembly.instantiate` which is slower. Original error:\n"", e);

                } else {
                    throw e;
                }
            }
        }

        const bytes = await module.arrayBuffer();
        return await WebAssembly.instantiate(bytes, imports);

    } else {
        const instance = await WebAssembly.instantiate(module, imports);

        if (instance instanceof WebAssembly.Instance) {
            return { instance, module };

        } else {
            return instance;
        }
    }
}

async function init(input) {
    if (typeof input === 'undefined') {
        input = new URL('rust_functions_bg.wasm', import.meta.url);
    }
    const imports = getImports();

    if (typeof input === 'string' || (typeof Request === 'function' && input instanceof Request) || (typeof URL === 'function' && input instanceof URL)) {
        input = fetch(input);
    }

    initMemory(imports);

    const { instance, module } = await load(await input, imports);

    return finalizeInit(instance, module);
} 

export { initSync }
export default init;
```

This is a bit more verbose than the previous example. The `init()` function gets the location of the Webassembly file, initialize memory for it, and then calls `load()`. `load()` streams the binary data into memory, instantiates it, and then returns the wasm object to be used with functions.  Unlike the Node version, the Browser version gives the developer the responsibility to call `init()` manually before calling any of the Webassembly functions.

Depending on the complexity of the build pipeline, this can cause some pain. Forgetting to call init before using the add() function results in an undefined error that isn't always the easiest to track down and understand. 

In addition, this code can **ONLY** run in the browser. If you're inside an SSR framework like Remix, trying to load your `.wasm` file in the server will error out, as Node does not have `instantiateStreaming()`, and it needs to be loaded into memory on the client, not the server. 

Understanding when files get loaded and how to control that in your framework is crucial to using Webassembly bundled this way.

Remix gives you a few choices in regard to how to use it. First, it must be initialized. This can be done alongside the function call inside of your component

```javascript
import init, {add} from ""../../../rust_functions/build/browser/rust_functions"";
import wasm from ""../../rust_functions/build/browser/rust_functions_bg.wasm"";

export default function Index() {
    // This useEffect prevents the add function from being called on the server
    useEffect(() => {
        init(wasm).then()  => {
            add(2,2)
        }, [])
    }
}
```

One of the neat tricks you can do in Webassembly is to load your WASM before hydration! This should decrease the time to interactivity, and make your UX more seamless. If I wasn't using the WASM across the entire site, I'd consider putting the below init() call inside of a conditional to prevent loading it for all pages. 

``` javascript
//entry.client.server.tsx
...
import init from ""../../rust_functions/build/browser/rust_functions"";
import wasm from ""../../rust_functions/build/browser/rust_functions_bg.wasm""

function hydrate() {
    React.startTransition(() => {
      init(wasm).then(() => hydrateRoot(
        document,
        <React.StrictMode>
          <RemixBrowser />
        </React.StrictMode>
      ))
    });
  }
  
  if (window.requestIdleCallback) {
    window.requestIdleCallback(hydrate);
  } else {
    window.setTimeout(hydrate, 1);
  }

// app/routes/index.tsx
export default function Index() {
    // Prevent WASM from running on the server
    useEffect(() => {
        add(2,2)
    }
```

In many ways, this is the simplest bundling method for static sites and sites that don't need to run in Node. The `init()` function can be called in a regular script tag, and the JS called as expected.

### Bundlers!

The third way I've seen Webassembly handled, and one that is outside the scope of my PR, is to use `wasm-pack --target bundler`. This generates a third kind of javascript wrapper, designed to be bundled by Webpack. Here we can see what the main JS file looks like when bundled this way.

```javascript
import * as wasm from ""./rust_functions_bg.wasm"";
export * from ""./rust_functions_bg.js"";
```

In this mode, it offloads the initialization of the Webassembly to the bundler itself, whcih has pluses and minuses. The bundler needs to understand and handle instantiation for the file. The [esbuild webassembly plugin](https://esbuild.github.io/plugins/#webassembly-plugin) talk about this process.

A stub module is created that wraps the wasm import from the file above, like so:

```javascript
import wasm from '/path/to/example.wasm'
export default (imports) =>
  WebAssembly.instantiate(wasm, imports).then(
    result => result.instance.exports)
```

Then esbuild creates another virtual module to hold the `.wasm` file's binary code, so that it can be called. Note that one would still need to do something like this in your Remix code:

```javascript
import load from './example.wasm'
load(imports).then(exports => { ... })
```

Much like the previous method, this plugin won't work for server side JS, necessitating modification of the plugin to determine where in your framework the WASM will get called. I never quite got this method to work in Remix, but if you do, feel free to drop me a DM on Twitter or Mastodon! I suspect that a more traditional SPA app, that doesn't do SSR, would fare better.

### Packaging Webassembly in NPM

Given the aforementioned WASM loading tricks, I'd like to propose a fairly simple way to package Webassembly so that it can be used by the most people. The previously mentioned [rust-remix repo](https://github.com/benwis/rust-remix)'s rust_functions package provides an example of this, but I'll cover the salient points below. 

Essentially we'll be providing two package sets, generated with both the `web` target for client side use, and the `node` target for server side use. This can bloat your npm package size, so if your users aren't likely to use a bundler, it may be worth splitting these into two packages.

```json
{
    ""name"": ""rust_functions"",
    ""version"": ""1.0.0"",
    ""license"": ""MIT"",
    ""scripts"": {
      ""build"": ""npm run build:browser && npm run build:node"",
      ""build:browser"": ""wasm-pack build --target web --out-dir ./build/browser && rimraf ./build/browser/package.json"",
      ""build:node"": ""wasm-pack build --target nodejs --out-dir ./build/node && rimraf ./build/node/package.json""
    },
    ""sideEffects"": false,
    ""files"": [
      ""build""
    ],
    ""types"": ""./build/browser/rust_functions.d.ts"",
    ""exports"": {
      ""."": {
        ""browser"": ""./build/browser/rust_functions.js"",
        ""node"": ""./build/node/rust_functions.js""
      },
      ""./binary.wasm"": {
        ""browser"": ""./build/browser/rust_functions_bg.wasm"",
        ""node"": ""./noop.js""
      }
    },
    ""devDependencies"": {
      ""rimraf"": ""^3.0.2""
    }
  }
```

Making use of a dual build script and a node's conditional imports, we can choose which generated js file is called depending on the environment. Remix's server environment for node based targets uses `node`, and the browser will load the `browser`. For the server, the JS will handle loading the `.wasm` file, and thus does not need to be called outside the package. For the browser environment, we need to make the `.wasm` file importable.

I've also spent some time experimenting with WASM packages on edge function environments like Deno Deploy. For that, you'll want to use the browser one, and be careful to load it 

We also need to remove the package.json files that are autogenerated by `wasm-pack` for both builds, so that we can control it. This allows us to use the parent one at our leisure.

## Performance Optimizations

Javascript is not a high performance language, and it likely never will be. Because of those limitations, it often tries to chear. At every opportunity, a smart JS developer will use a Browser API or native OS library to do high performance tasks. Computationally intensive tasks include, but are not limited to image manipulation, encryption, hashing and compression, large amounts of JSON processing, and 2D/3D graphics.

This has often limited architecture choices. For example, if image resizing and transcoding is expensive to do in the browser, we're forced to do it on a server somewhere instead. The push for edge computing has limited the ability to use this escape hatch, although workarounds for that have been found.

In my Remix talk, I posited the idea of Rust/Webassembly as the final level of ""progressive enhancement"". Because sometimes JS just isn't fast enough, we should consider whether Webassembly might enable us to give a better user experience. 

As an example of some of the possible performance benefits, I'm going to do a little microbenchmark. Let's say you want to generate a SHA256 hash in the browser or on the edge when a user uploads a file.  

We'll compare the performance of Rust's `sha2` crate compiled to Webassembly and Amazon's `@aws-crypto/sha256-js `package, which is a pure JS implementation. In this test, we'll hash a 5MB file with both in a Remix loader, which runs in Node. It was run ten times in a local environment, and the results were averaged.

<figure>
<img style=""height: 30rem; margin: auto;"" src=""https://benwis.imgix.net/5mb_sha256_chart.png?auto=format,compress"" alt=""Bar chart of 5MB SHA256 Hash Times"">
<figcaption style=""text-align: center;"">Averaged over 10 runs</figcaption>
</figure>

Using Webassembly for this application net an approximately 234% reduction in hash time. That can have real implications for page load time and server usage under load.  For those curious, the Remix code looks like this:

```typescript
export const loader: LoaderFunction = async ({ request }) => {
  const buf = randomBytes(50000000);
//JS Reference Implemntation
  const jsStart = Date.now()
  const hasher = new Sha256();
  hasher.update(buf);
  const rawJsHash = await hasher.digest();
  const jsHash = btoa(String.fromCharCode.apply(null, rawJsHash));
  const jsEnd = Date.now()
  const jsTime = jsEnd - jsStart;

  // Rust -> WASM Implementation using wasm-bindgen and node
  const wasmStart = Date.now();
  const hash2 = hash_wasm_server(buf);
  const wasmEnd = Date.now();
  const wasmTime = wasmEnd - wasmStart;
}
```

The rust function `hash_wasm_server()` is pretty close to the example, and is defined thusly:

```rust
#[wasm_bindgen]
pub fn hash_wasm_server(buf: Uint8Array) -> String {
    let mut hasher = Sha256::new();
    let owned_buf = buf.to_vec();
    hasher.update(owned_buf);
    let hash = hasher.finalize();
    Base64::encode_string(&hash)
}
```

I'm hardly the only person to consider this approach. Discourse published a [nice blog post](https://blog.discourse.org/2021/07/faster-user-uploads-on-discourse-with-rust-webassembly-and-mozjpeg/) about how they optimized image processing for their forum software using Rust and WebAssembly. There are definitely some caveats to this idea, it's not a panacea, but for certain applications the performance benefits are very, very real.

## Games

I remember the heady days of Flash gaming back in the early aughts, when I was in middle school, back when people had iPods and were just starting to experience AutoTune. Yes, back in the aughts (2000-2010ish) I played a ton of flash games in school. Games like [QWOP](https://www.foddy.net/Athletics.html) and game sites like Popcap and Newgrounds ruled my childhood. The rise of Rust and it's native compilation into WebAssembly have allowed an entirely new generation of game engines to target the browser. Notable examples among these are [Bevy](https://bevyengine.org/), which is capable of both 2d and 3d gaming, and [Macroquad](https://macroquad.rs), which is simpler to use, have enabled a new generation of these games to spring up.

Since you've come this far, maybe it's time to take a break and check out some of the lovely Rust Webassembly games on [itch.io](https://itch.io/games/platform-web/tag-bevy). I particularily enjoyed playing some of the [Bevy Jam 2 submissions](https://itch.io/jam/bevy-jam-2) like [Elemental Sorcerer](https://itch.io/jam/bevy-jam-2/rate/1679806) and [Shanty Quest Treble at Sea](https://itch.io/jam/bevy-jam-2/rate/1679538) With Bevy's [recent release](https://bevyengine.org/news/bevy-0-9/) of 0.9, it's never been a better time to start making games!

<figure>
<img style=""height: 25rem; margin: auto;"" src=""https://img.itch.zone/aW1nLzk5MTIyMDUucG5n/315x250%23c/cKiTcm.png"" alt=""Shanty Quest Treble At Sea"">
<figcaption style=""text-align: center;"">I totally didn't bass my opinion on the thumbnails</figcaption>
</figure>

## Conclusion

Hopefully we all know a little bit more about how Javascript and Webassembly interact, and how use it from within Remix. I'm pretty excited about how Webassembly is progressing, and how it can enable new projects and better user experiences. ",,webassembly-on-remix,1,1,"",,,1665619200,1680129527
